{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GhostBusters: AML Mule Account Detection - A100 GPU Training Pipeline\n",
    "\n",
    "**Anti-Money Laundering (AML) Fraud Detection using Graph Neural Networks**\n",
    "\n",
    "This notebook trains multiple ML and GNN models on the AMLSim dataset to detect mule/SAR accounts.\n",
    "\n",
    "### Pipeline Overview\n",
    "1. **Environment Setup** - A100 GPU detection, MIG 1g.5gb optimization, dependency checks\n",
    "2. **Dataset Verification** - Validate all train/val/test splits and graph data\n",
    "3. **Data Loading & Preprocessing** - Load numpy arrays, normalize features\n",
    "4. **Baseline Models** - XGBoost, Random Forest, Logistic Regression, Gradient Boosting\n",
    "5. **GNN Models** - GraphSAGE, GAT, GCN, GIN, HeteroGNN\n",
    "6. **Model Saving** - PyTorch (.pt), TorchScript, ONNX, Pickle (sklearn/xgb)\n",
    "7. **Visualizations** - Training curves, graph structure, embeddings, attention, comparisons\n",
    "\n",
    "### Hardware Target\n",
    "- NVIDIA A100 GPU (1g.5gb MIG instance or full GPU)\n",
    "- Mixed precision (FP16/BF16) with TF32 matmul\n",
    "- Fused Adam optimizer, torch.compile, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# ---- PyTorch ----\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Environment Setup & A100 GPU Configuration\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---- PyTorch ----\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "# ---- Sklearn ----\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, precision_recall_curve, auc,\n",
    "    f1_score, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, roc_curve\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ---- XGBoost ----\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "    print('[WARN] XGBoost not installed. pip install xgboost')\n",
    "\n",
    "# ---- Visualization ----\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend for remote servers\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_theme(style='whitegrid', font_scale=1.1)\n",
    "    HAS_SNS = True\n",
    "except ImportError:\n",
    "    HAS_SNS = False\n",
    "\n",
    "try:\n",
    "    import networkx as nx\n",
    "    HAS_NX = True\n",
    "except ImportError:\n",
    "    HAS_NX = False\n",
    "    print('[WARN] networkx not installed. pip install networkx')\n",
    "\n",
    "print('Core imports loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 18 21:54:39 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.36                 Driver Version: 566.36         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   54C    P8             14W /   88W |    2458MiB /   6144MiB |     16%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3084    C+G   ...nr4m\\radeonsoftware\\AMDRSSrcExt.exe      N/A      |\n",
      "|    0   N/A  N/A      3160    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      3940    C+G   ...117_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A      5232    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      5460    C+G   ...117_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A      6744    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      7232    C+G   ...cw5n1h2txyewy\\CrossDeviceResume.exe      N/A      |\n",
      "|    0   N/A  N/A      7968    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12796    C+G   ...n\\144.0.3719.115\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     15280    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A     15288    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17112    C+G   ...n\\144.0.3719.115\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     18180    C+G   ...64__cv1g1gvanyjgm\\WhatsApp.Root.exe      N/A      |\n",
      "|    0   N/A  N/A     18240    C+G   ...m\\radeonsoftware\\RadeonSoftware.exe      N/A      |\n",
      "|    0   N/A  N/A     18392    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     18964    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     19220    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     20912    C+G   ...s\\System32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A     20920    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     21964    C+G   ...84.0_x64__8wekyb3d8bbwe\\Copilot.exe      N/A      |\n",
      "|    0   N/A  N/A     24008    C+G   ...n\\144.0.3719.115\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     25516    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     26264    C+G   ...n\\144.0.3719.115\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     26284    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A     28940      C   C:\\Python313\\python.exe                     N/A      |\n",
      "|    0   N/A  N/A     29244    C+G   ...rograms\\Antigravity\\Antigravity.exe      N/A      |\n",
      "|    0   N/A  N/A     30388    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     39816    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     41976    C+G   ...rograms\\Antigravity\\Antigravity.exe      N/A      |\n",
      "|    0   N/A  N/A     43760    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: A100 GPU Detection & MIG 1g.5gb Configuration\n",
    "# ============================================================\n",
    "\n",
    "def setup_a100_optimizations():\n",
    "    \"\"\"Configure PyTorch for optimal A100 performance (including MIG 1g.5gb).\"\"\"\n",
    "    config = {\n",
    "        'device': 'cpu',\n",
    "        'gpu_name': 'N/A',\n",
    "        'gpu_memory_gb': 0,\n",
    "        'is_a100': False,\n",
    "        'is_mig': False,\n",
    "        'mig_profile': 'N/A',\n",
    "        'use_amp': False,\n",
    "        'use_bf16': False,\n",
    "        'use_tf32': False,\n",
    "        'use_compile': False,\n",
    "        'use_fused_adam': False,\n",
    "        'batch_size': 32768,\n",
    "        'hidden_dim': 128,\n",
    "        'num_workers': 2,\n",
    "    }\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print('WARNING: CUDA not available. Using CPU (training will be slow).')\n",
    "        return config\n",
    "    \n",
    "    config['device'] = 'cuda'\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    config['gpu_name'] = gpu_name\n",
    "    config['gpu_memory_gb'] = round(gpu_mem, 1)\n",
    "    \n",
    "    print(f'GPU Detected: {gpu_name}')\n",
    "    print(f'GPU Memory: {gpu_mem:.1f} GB')\n",
    "    print(f'CUDA Version: {torch.version.cuda}')\n",
    "    print(f'PyTorch Version: {torch.__version__}')\n",
    "    \n",
    "    # Detect A100\n",
    "    config['is_a100'] = 'A100' in gpu_name.upper()\n",
    "    \n",
    "    # Detect MIG instance (A100 MIG profiles have reduced memory)\n",
    "    # 1g.5gb = ~5GB, 2g.10gb = ~10GB, 3g.20gb = ~20GB, 7g.40gb/80gb = full\n",
    "    if config['is_a100']:\n",
    "        if gpu_mem < 7:\n",
    "            config['is_mig'] = True\n",
    "            config['mig_profile'] = '1g.5gb'\n",
    "            config['batch_size'] = 16384\n",
    "            config['hidden_dim'] = 128\n",
    "            config['num_workers'] = 1\n",
    "            print('MIG Profile: 1g.5gb (5GB slice)')\n",
    "        elif gpu_mem < 12:\n",
    "            config['is_mig'] = True\n",
    "            config['mig_profile'] = '2g.10gb'\n",
    "            config['batch_size'] = 32768\n",
    "            config['hidden_dim'] = 192\n",
    "            config['num_workers'] = 2\n",
    "            print('MIG Profile: 2g.10gb (10GB slice)')\n",
    "        elif gpu_mem < 25:\n",
    "            config['is_mig'] = True\n",
    "            config['mig_profile'] = '3g.20gb'\n",
    "            config['batch_size'] = 65536\n",
    "            config['hidden_dim'] = 256\n",
    "            config['num_workers'] = 4\n",
    "            print('MIG Profile: 3g.20gb (20GB slice)')\n",
    "        else:\n",
    "            config['mig_profile'] = 'full'\n",
    "            config['batch_size'] = 131072\n",
    "            config['hidden_dim'] = 512\n",
    "            config['num_workers'] = 4\n",
    "            print('Full A100 GPU detected (40GB or 80GB)')\n",
    "    else:\n",
    "        # Non-A100 GPU - adapt based on memory\n",
    "        if gpu_mem < 8:\n",
    "            config['batch_size'] = 16384\n",
    "            config['hidden_dim'] = 128\n",
    "        elif gpu_mem < 16:\n",
    "            config['batch_size'] = 32768\n",
    "            config['hidden_dim'] = 192\n",
    "        else:\n",
    "            config['batch_size'] = 65536\n",
    "            config['hidden_dim'] = 256\n",
    "    \n",
    "    # Enable TF32 for A100 (faster FP32 matmul with TF32 precision)\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    config['use_tf32'] = True\n",
    "    print('TF32 matmul: ENABLED')\n",
    "    \n",
    "    # Mixed precision (AMP)\n",
    "    config['use_amp'] = True\n",
    "    print('Mixed Precision (AMP): ENABLED')\n",
    "    \n",
    "    # BF16 support (A100 has native BF16)\n",
    "    if config['is_a100'] or torch.cuda.is_bf16_supported():\n",
    "        config['use_bf16'] = True\n",
    "        print('BF16 Support: ENABLED')\n",
    "    \n",
    "    # torch.compile (PyTorch 2.0+)\n",
    "    if hasattr(torch, 'compile'):\n",
    "        config['use_compile'] = True\n",
    "        print('torch.compile: AVAILABLE')\n",
    "    \n",
    "    # Fused Adam optimizer\n",
    "    config['use_fused_adam'] = True\n",
    "    print('Fused Adam: ENABLED')\n",
    "    \n",
    "    # CUDA optimizations\n",
    "    torch.cuda.empty_cache()\n",
    "    if hasattr(torch.cuda, 'memory_stats'):\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # cuDNN benchmark mode for consistent input sizes\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print('cuDNN benchmark: ENABLED')\n",
    "    \n",
    "    print(f'\\nOptimal batch_size: {config[\"batch_size\"]}')\n",
    "    print(f'Optimal hidden_dim: {config[\"hidden_dim\"]}')\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Run setup\n",
    "GPU_CONFIG = setup_a100_optimizations()\n",
    "DEVICE = torch.device(GPU_CONFIG['device'])\n",
    "print(f'\\nUsing device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Project Paths & Directory Setup\n",
    "# ============================================================\n",
    "\n",
    "# Auto-detect project root (works in Jupyter on remote server)\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "PROJECT_ROOT = NOTEBOOK_DIR  # Notebook is at project root\n",
    "\n",
    "BASE_DIR = os.path.join(PROJECT_ROOT, 'data', 'amlsim_v1')\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'results')\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "VIZ_DIR = os.path.join(PROJECT_ROOT, 'visualizations')\n",
    "SCRIPTS_DIR = os.path.join(PROJECT_ROOT, 'scripts')\n",
    "\n",
    "# Create output directories\n",
    "for d in [RESULTS_DIR, MODELS_DIR, VIZ_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f'Project Root: {PROJECT_ROOT}')\n",
    "print(f'Data Dir:     {BASE_DIR}')\n",
    "print(f'Models Dir:   {MODELS_DIR}')\n",
    "print(f'Results Dir:  {RESULTS_DIR}')\n",
    "print(f'Viz Dir:      {VIZ_DIR}')\n",
    "print(f'\\nAll output directories ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Dataset Verification - Check ALL files exist\n",
    "# ============================================================\n",
    "\n",
    "def verify_dataset():\n",
    "    \"\"\"Comprehensive dataset verification across all splits.\"\"\"\n",
    "    \n",
    "    splits = ['train', 'val', 'test']\n",
    "    \n",
    "    # Files expected in each split directory\n",
    "    csv_files = [\n",
    "        'accounts.csv', 'transactions.csv', 'sar_accounts.csv',\n",
    "        'alert_accounts.csv', 'enriched_transactions.csv',\n",
    "        'app_logins.csv', 'atm_withdrawals.csv', 'wallet_links.csv',\n",
    "        'upi_handles.csv', 'individuals-bulkload.csv', 'organizations-bulkload.csv'\n",
    "    ]\n",
    "    \n",
    "    # Graph data files\n",
    "    graph_files = [\n",
    "        'account_features.npy', 'account_labels.npy',\n",
    "        'edge_features.npy', 'edge_labels.npy',\n",
    "        'transfer_edge_index.npy', 'same_wallet_edge_index.npy',\n",
    "        'wallet_edge_index.npy', 'vpa_edge_index.npy',\n",
    "        'atm_edge_index.npy', 'bank_edge_index.npy',\n",
    "        'login_edge_index.npy', 'shared_device_edge_index.npy',\n",
    "        'graph_stats.json', 'id_mappings.json'\n",
    "    ]\n",
    "    \n",
    "    all_ok = True\n",
    "    summary = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        split_dir = os.path.join(BASE_DIR, split)\n",
    "        graph_dir = os.path.join(split_dir, 'graph_data')\n",
    "        \n",
    "        print(f'\\n{\"=\"*60}')\n",
    "        print(f'  Verifying: {split.upper()} split')\n",
    "        print(f'{\"=\"*60}')\n",
    "        \n",
    "        split_info = {'csv': {}, 'graph': {}, 'missing': []}\n",
    "        \n",
    "        # Check CSV files\n",
    "        for f in csv_files:\n",
    "            path = os.path.join(split_dir, f)\n",
    "            if os.path.exists(path):\n",
    "                size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "                try:\n",
    "                    df = pd.read_csv(path, nrows=1)\n",
    "                    nrows = sum(1 for _ in open(path)) - 1\n",
    "                    split_info['csv'][f] = {'rows': nrows, 'cols': len(df.columns), 'size_mb': round(size_mb, 1)}\n",
    "                    print(f'  OK  {f:<35} {nrows:>10,} rows  {size_mb:>7.1f} MB')\n",
    "                except Exception:\n",
    "                    split_info['csv'][f] = {'size_mb': round(size_mb, 1)}\n",
    "                    print(f'  OK  {f:<35} {size_mb:>7.1f} MB')\n",
    "            else:\n",
    "                split_info['missing'].append(f)\n",
    "                print(f'  MISSING  {f}')\n",
    "                all_ok = False\n",
    "        \n",
    "        # Check graph data files\n",
    "        print(f'\\n  Graph Data:')\n",
    "        for f in graph_files:\n",
    "            path = os.path.join(graph_dir, f)\n",
    "            if os.path.exists(path):\n",
    "                size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "                if f.endswith('.npy'):\n",
    "                    arr = np.load(path)\n",
    "                    split_info['graph'][f] = {'shape': list(arr.shape), 'dtype': str(arr.dtype), 'size_mb': round(size_mb, 1)}\n",
    "                    print(f'  OK  {f:<35} shape={str(arr.shape):<20} {size_mb:>7.1f} MB')\n",
    "                else:\n",
    "                    split_info['graph'][f] = {'size_mb': round(size_mb, 1)}\n",
    "                    print(f'  OK  {f:<35} {size_mb:>7.1f} MB')\n",
    "            else:\n",
    "                split_info['missing'].append(f'graph_data/{f}')\n",
    "                print(f'  MISSING  graph_data/{f}')\n",
    "                all_ok = False\n",
    "        \n",
    "        # Load and display graph stats\n",
    "        stats_path = os.path.join(graph_dir, 'graph_stats.json')\n",
    "        if os.path.exists(stats_path):\n",
    "            with open(stats_path) as sf:\n",
    "                stats = json.load(sf)\n",
    "            print(f'\\n  Graph Statistics:')\n",
    "            print(f'    Accounts: {stats[\"num_accounts\"]:,} ({stats[\"num_sar_accounts\"]:,} SAR)')\n",
    "            print(f'    Total edges: {stats[\"total_edges\"]:,}')\n",
    "            sar_rate = stats[\"num_sar_accounts\"] / max(stats[\"num_accounts\"], 1) * 100\n",
    "            print(f'    SAR rate: {sar_rate:.2f}%')\n",
    "        \n",
    "        summary[split] = split_info\n",
    "    \n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    if all_ok:\n",
    "        print('  DATASET VERIFICATION: ALL FILES PRESENT')\n",
    "    else:\n",
    "        print('  DATASET VERIFICATION: SOME FILES MISSING (see above)')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    return summary, all_ok\n",
    "\n",
    "dataset_summary, dataset_ok = verify_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Data Loading & Preprocessing\n",
    "# ============================================================\n",
    "\n",
    "def load_split(split):\n",
    "    \"\"\"Load all graph arrays for one split.\"\"\"\n",
    "    gd = os.path.join(BASE_DIR, split, 'graph_data')\n",
    "    data = {}\n",
    "    arrays = [\n",
    "        'account_features', 'account_labels', 'edge_features', 'edge_labels',\n",
    "        'transfer_edge_index', 'same_wallet_edge_index',\n",
    "        'wallet_edge_index', 'vpa_edge_index', 'atm_edge_index',\n",
    "        'bank_edge_index', 'login_edge_index', 'shared_device_edge_index'\n",
    "    ]\n",
    "    for name in arrays:\n",
    "        path = os.path.join(gd, f'{name}.npy')\n",
    "        if os.path.exists(path):\n",
    "            data[name] = np.load(path)\n",
    "    return data\n",
    "\n",
    "def prepare_node_data(train_d, val_d, test_d):\n",
    "    \"\"\"Prepare node classification tensors with combined adjacency.\"\"\"\n",
    "    X_train = torch.tensor(train_d['account_features'], dtype=torch.float32).to(DEVICE)\n",
    "    y_train = torch.tensor(train_d['account_labels'], dtype=torch.long).to(DEVICE)\n",
    "    X_val = torch.tensor(val_d['account_features'], dtype=torch.float32).to(DEVICE)\n",
    "    y_val = torch.tensor(val_d['account_labels'], dtype=torch.long).to(DEVICE)\n",
    "    X_test = torch.tensor(test_d['account_features'], dtype=torch.float32).to(DEVICE)\n",
    "    y_test = torch.tensor(test_d['account_labels'], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "    # Handle NaN/Inf\n",
    "    for X in [X_train, X_val, X_test]:\n",
    "        X[torch.isnan(X)] = 0\n",
    "        X[torch.isinf(X)] = 0\n",
    "\n",
    "    # Normalize features using train set statistics\n",
    "    mean = X_train.mean(dim=0)\n",
    "    std = X_train.std(dim=0).clamp(min=1e-6)\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_val = (X_val - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "\n",
    "    # Build combined adjacency: transfer + same_wallet + shared_device\n",
    "    def build_edges(d):\n",
    "        edges_list = []\n",
    "        for key in ['transfer_edge_index', 'same_wallet_edge_index', 'shared_device_edge_index']:\n",
    "            if key in d and d[key].shape[1] > 0:\n",
    "                edges_list.append(torch.tensor(d[key], dtype=torch.long))\n",
    "        if edges_list:\n",
    "            return torch.cat(edges_list, dim=1).to(DEVICE)\n",
    "        return torch.tensor(d['transfer_edge_index'], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "    edge_index_train = build_edges(train_d)\n",
    "    edge_index_val = build_edges(val_d)\n",
    "    edge_index_test = build_edges(test_d)\n",
    "\n",
    "    return {\n",
    "        'X_train': X_train, 'y_train': y_train, 'edge_index_train': edge_index_train,\n",
    "        'X_val': X_val, 'y_val': y_val, 'edge_index_val': edge_index_val,\n",
    "        'X_test': X_test, 'y_test': y_test, 'edge_index_test': edge_index_test,\n",
    "        'mean': mean, 'std': std\n",
    "    }\n",
    "\n",
    "# Load data\n",
    "print('Loading graph data from all splits...')\n",
    "t0 = time.time()\n",
    "train_d = load_split('train')\n",
    "val_d = load_split('val')\n",
    "test_d = load_split('test')\n",
    "print(f'  Raw data loaded in {time.time()-t0:.1f}s')\n",
    "\n",
    "# Prepare tensors\n",
    "print('Preparing GPU tensors...')\n",
    "t0 = time.time()\n",
    "node_data = prepare_node_data(train_d, val_d, test_d)\n",
    "in_dim = node_data['X_train'].size(1)\n",
    "print(f'  Tensors ready in {time.time()-t0:.1f}s')\n",
    "print(f'  Input features: {in_dim}')\n",
    "print(f'  Train: {node_data[\"X_train\"].shape[0]:,} nodes, {node_data[\"edge_index_train\"].shape[1]:,} edges')\n",
    "print(f'  Val:   {node_data[\"X_val\"].shape[0]:,} nodes, {node_data[\"edge_index_val\"].shape[1]:,} edges')\n",
    "print(f'  Test:  {node_data[\"X_test\"].shape[0]:,} nodes, {node_data[\"edge_index_test\"].shape[1]:,} edges')\n",
    "print(f'  Class balance (train): {(node_data[\"y_train\"]==1).sum().item():,} SAR / {(node_data[\"y_train\"]==0).sum().item():,} normal')\n",
    "\n",
    "# Memory report\n",
    "if torch.cuda.is_available():\n",
    "    mem_alloc = torch.cuda.memory_allocated() / 1024**3\n",
    "    mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f'  GPU Memory: {mem_alloc:.2f} / {mem_total:.1f} GB allocated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Metrics Computation Utility\n",
    "# ============================================================\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    'acct_type', 'is_active', 'prior_sar', 'initial_deposit',\n",
    "    'sent_count', 'recv_count', 'sent_amt_mean', 'sent_amt_std',\n",
    "    'recv_amt_mean', 'recv_amt_std', 'fan_out_ratio', 'fan_in_ratio',\n",
    "    'recv_send_delay', 'channel_diversity'\n",
    "]\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"Compute comprehensive classification metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    metrics['accuracy'] = report['accuracy']\n",
    "    metrics['f1_macro'] = report['macro avg']['f1-score']\n",
    "    metrics['f1_weighted'] = report['weighted avg']['f1-score']\n",
    "\n",
    "    if '1' in report:\n",
    "        metrics['precision_sar'] = report['1']['precision']\n",
    "        metrics['recall_sar'] = report['1']['recall']\n",
    "        metrics['f1_sar'] = report['1']['f1-score']\n",
    "\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "            metrics['pr_auc'] = float(auc(rec, prec))\n",
    "            metrics['avg_precision'] = float(average_precision_score(y_true, y_prob))\n",
    "            metrics['roc_auc'] = float(roc_auc_score(y_true, y_prob))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['confusion_matrix'] = cm.tolist()\n",
    "\n",
    "    n_pos = int(y_true.sum())\n",
    "    for k_mult in [1, 2, 5]:\n",
    "        k = n_pos * k_mult\n",
    "        if y_prob is not None and k <= len(y_prob):\n",
    "            top_k_idx = np.argsort(y_prob)[-k:]\n",
    "            recall_at_k = y_true[top_k_idx].sum() / max(n_pos, 1)\n",
    "            metrics[f'recall@{k_mult}x'] = float(recall_at_k)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, name):\n",
    "    \"\"\"Print formatted metrics.\"\"\"\n",
    "    print(f'\\n  === {name} ===')\n",
    "    print(f'  PR-AUC:     {metrics.get(\"pr_auc\", 0):.4f}')\n",
    "    print(f'  ROC-AUC:    {metrics.get(\"roc_auc\", 0):.4f}')\n",
    "    print(f'  F1 (SAR):   {metrics.get(\"f1_sar\", 0):.4f}')\n",
    "    print(f'  Precision:  {metrics.get(\"precision_sar\", 0):.4f}')\n",
    "    print(f'  Recall:     {metrics.get(\"recall_sar\", 0):.4f}')\n",
    "    if 'recall@1x' in metrics:\n",
    "        print(f'  Recall@1x:  {metrics[\"recall@1x\"]:.4f}')\n",
    "        print(f'  Recall@2x:  {metrics.get(\"recall@2x\", 0):.4f}')\n",
    "    cm = metrics.get('confusion_matrix', [[0,0],[0,0]])\n",
    "    print(f'  Confusion:  TN={cm[0][0]:,} FP={cm[0][1]:,} | FN={cm[1][0]:,} TP={cm[1][1]:,}')\n",
    "\n",
    "print('Metrics utilities loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Baseline Models (XGBoost, Random Forest, Gradient Boosting, Logistic Regression)\n",
    "\n",
    "Traditional ML models trained on the 14-dimensional account feature vectors. These serve as strong baselines to compare against GNN approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Train ALL Baseline Models + Save\n",
    "# ============================================================\n",
    "\n",
    "def train_all_baselines():\n",
    "    \"\"\"Train XGBoost, Random Forest, Gradient Boosting, Logistic Regression.\"\"\"\n",
    "    print('='*60)\n",
    "    print('BASELINE MODELS - Node Classification (Mule/SAR Detection)')\n",
    "    print('='*60)\n",
    "    \n",
    "    # Prepare numpy data\n",
    "    X_train = train_d['account_features'].copy()\n",
    "    y_train = train_d['account_labels'].copy()\n",
    "    X_val = val_d['account_features'].copy()\n",
    "    y_val = val_d['account_labels'].copy()\n",
    "    X_test = test_d['account_features'].copy()\n",
    "    y_test = test_d['account_labels'].copy()\n",
    "    \n",
    "    for X in [X_train, X_val, X_test]:\n",
    "        X[np.isnan(X)] = 0\n",
    "        X[np.isinf(X)] = 0\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "    \n",
    "    n_pos = y_train.sum()\n",
    "    n_neg = len(y_train) - n_pos\n",
    "    scale_pos = n_neg / max(n_pos, 1)\n",
    "    print(f'  Train: {len(y_train):,} ({n_pos:,} SAR, {n_neg:,} normal, ratio 1:{scale_pos:.1f})')\n",
    "    \n",
    "    results = {}\n",
    "    models = {}\n",
    "    \n",
    "    # 1. XGBoost\n",
    "    if HAS_XGB:\n",
    "        print('\\n--- Training XGBoost ---')\n",
    "        t0 = time.time()\n",
    "        xgb = XGBClassifier(\n",
    "            n_estimators=300, max_depth=6, learning_rate=0.1,\n",
    "            scale_pos_weight=scale_pos, eval_metric='aucpr',\n",
    "            use_label_encoder=False, random_state=42, n_jobs=-1,\n",
    "            tree_method='hist',  # Fast histogram method (GPU-compatible)\n",
    "        )\n",
    "        xgb.fit(X_train_s, y_train, eval_set=[(X_val_s, y_val)], verbose=False)\n",
    "        y_pred = xgb.predict(X_test_s)\n",
    "        y_prob = xgb.predict_proba(X_test_s)[:, 1]\n",
    "        m = compute_metrics(y_test, y_pred, y_prob)\n",
    "        print_metrics(m, f'XGBoost ({time.time()-t0:.1f}s)')\n",
    "        results['xgboost'] = m\n",
    "        models['xgboost'] = xgb\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    print('\\n--- Training Random Forest ---')\n",
    "    t0 = time.time()\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=10, class_weight='balanced',\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_s, y_train)\n",
    "    y_pred_rf = rf.predict(X_test_s)\n",
    "    y_prob_rf = rf.predict_proba(X_test_s)[:, 1]\n",
    "    m_rf = compute_metrics(y_test, y_pred_rf, y_prob_rf)\n",
    "    print_metrics(m_rf, f'Random Forest ({time.time()-t0:.1f}s)')\n",
    "    results['random_forest'] = m_rf\n",
    "    models['random_forest'] = rf\n",
    "    \n",
    "    # 3. Gradient Boosting (sklearn)\n",
    "    print('\\n--- Training Gradient Boosting ---')\n",
    "    t0 = time.time()\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "        subsample=0.8, random_state=42\n",
    "    )\n",
    "    gb.fit(X_train_s, y_train)\n",
    "    y_pred_gb = gb.predict(X_test_s)\n",
    "    y_prob_gb = gb.predict_proba(X_test_s)[:, 1]\n",
    "    m_gb = compute_metrics(y_test, y_pred_gb, y_prob_gb)\n",
    "    print_metrics(m_gb, f'Gradient Boosting ({time.time()-t0:.1f}s)')\n",
    "    results['gradient_boosting'] = m_gb\n",
    "    models['gradient_boosting'] = gb\n",
    "    \n",
    "    # 4. Logistic Regression\n",
    "    print('\\n--- Training Logistic Regression ---')\n",
    "    t0 = time.time()\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "    lr.fit(X_train_s, y_train)\n",
    "    y_pred_lr = lr.predict(X_test_s)\n",
    "    y_prob_lr = lr.predict_proba(X_test_s)[:, 1]\n",
    "    m_lr = compute_metrics(y_test, y_pred_lr, y_prob_lr)\n",
    "    print_metrics(m_lr, f'Logistic Regression ({time.time()-t0:.1f}s)')\n",
    "    results['logistic_regression'] = m_lr\n",
    "    models['logistic_regression'] = lr\n",
    "    \n",
    "    # Save all baseline models\n",
    "    print('\\n--- Saving Baseline Models ---')\n",
    "    \n",
    "    # Save scaler (needed for inference)\n",
    "    scaler_path = os.path.join(MODELS_DIR, 'baseline_scaler.pkl')\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f'  Saved: {scaler_path}')\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Pickle format\n",
    "        pkl_path = os.path.join(MODELS_DIR, f'baseline_{name}.pkl')\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f'  Saved: {pkl_path}')\n",
    "    \n",
    "    # Save XGBoost native format\n",
    "    if HAS_XGB and 'xgboost' in models:\n",
    "        xgb_path = os.path.join(MODELS_DIR, 'xgboost_native.json')\n",
    "        models['xgboost'].save_model(xgb_path)\n",
    "        print(f'  Saved: {xgb_path} (native XGBoost format)')\n",
    "    \n",
    "    # Store predictions for visualization later\n",
    "    baseline_preds = {\n",
    "        'y_test': y_test,\n",
    "        'xgboost': {'prob': y_prob if HAS_XGB else None, 'pred': y_pred if HAS_XGB else None},\n",
    "        'random_forest': {'prob': y_prob_rf, 'pred': y_pred_rf},\n",
    "        'gradient_boosting': {'prob': y_prob_gb, 'pred': y_pred_gb},\n",
    "        'logistic_regression': {'prob': y_prob_lr, 'pred': y_pred_lr},\n",
    "    }\n",
    "    \n",
    "    return results, models, baseline_preds\n",
    "\n",
    "baseline_results, baseline_models, baseline_preds = train_all_baselines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: GNN Model Definitions\n",
    "\n",
    "Five Graph Neural Network architectures optimized for A100:\n",
    "1. **GraphSAGE** - Mean aggregation with self-loop\n",
    "2. **GAT** - Multi-head attention (4 heads)\n",
    "3. **GCN** - Spectral graph convolution (symmetric normalization)\n",
    "4. **GIN** - Graph Isomorphism Network (sum aggregation + MLP)\n",
    "5. **HeteroGNN** - Multi-relation heterogeneous GNN using all edge types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: GNN Layer & Model Definitions (all 5 architectures)\n",
    "# ============================================================\n",
    "\n",
    "# ---------- Layer Definitions ----------\n",
    "\n",
    "class SAGEConv(nn.Module):\n",
    "    \"\"\"GraphSAGE convolution (mean aggregation).\"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear_self = nn.Linear(in_dim, out_dim)\n",
    "        self.linear_neigh = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "        N = x.size(0)\n",
    "        neigh_sum = torch.zeros(N, x.size(1), device=x.device, dtype=x.dtype)\n",
    "        neigh_count = torch.zeros(N, 1, device=x.device, dtype=x.dtype)\n",
    "        neigh_sum.index_add_(0, dst, x[src])\n",
    "        neigh_count.index_add_(0, dst, torch.ones(src.size(0), 1, device=x.device, dtype=x.dtype))\n",
    "        neigh_mean = neigh_sum / neigh_count.clamp(min=1)\n",
    "        return self.linear_self(x) + self.linear_neigh(neigh_mean)\n",
    "\n",
    "\n",
    "class GATConvLayer(nn.Module):\n",
    "    \"\"\"Graph Attention convolution (multi-head).\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = out_dim // num_heads\n",
    "        assert out_dim % num_heads == 0\n",
    "        self.W = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.a_src = nn.Parameter(torch.zeros(num_heads, self.head_dim))\n",
    "        self.a_dst = nn.Parameter(torch.zeros(num_heads, self.head_dim))\n",
    "        nn.init.xavier_uniform_(self.a_src.unsqueeze(0))\n",
    "        nn.init.xavier_uniform_(self.a_dst.unsqueeze(0))\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        N = x.size(0)\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "        E = src.size(0)\n",
    "        h = self.W(x).view(N, self.num_heads, self.head_dim)\n",
    "        score_src = (h * self.a_src).sum(dim=-1)\n",
    "        score_dst = (h * self.a_dst).sum(dim=-1)\n",
    "        e = self.leaky_relu(score_src[src] + score_dst[dst])\n",
    "        e_max = torch.zeros(N, self.num_heads, device=x.device, dtype=e.dtype)\n",
    "        e_max.index_reduce_(0, dst, e, 'amax', include_self=True)\n",
    "        e_exp = torch.exp(e - e_max[dst])\n",
    "        e_sum = torch.zeros(N, self.num_heads, device=x.device, dtype=e.dtype)\n",
    "        e_sum.index_add_(0, dst, e_exp)\n",
    "        alpha = self.dropout(e_exp / e_sum[dst].clamp(min=1e-9))\n",
    "        # Chunked aggregation to avoid OOM on large graphs (3.4M+ edges)\n",
    "        CHUNK = 500_000\n",
    "        out = torch.zeros(N, self.num_heads, self.head_dim, device=x.device, dtype=torch.float32)\n",
    "        for i in range(0, E, CHUNK):\n",
    "            j = min(i + CHUNK, E)\n",
    "            msg_chunk = h[src[i:j]].float() * alpha[i:j].unsqueeze(-1).float()\n",
    "            out.index_add_(0, dst[i:j], msg_chunk)\n",
    "        return out.reshape(N, -1)\n",
    "\n",
    "\n",
    "class GCNConv(nn.Module):\n",
    "    \"\"\"Graph Convolutional Network layer (symmetric normalization).\"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "        N = x.size(0)\n",
    "        # Compute degree for symmetric normalization: D^{-1/2} A D^{-1/2}\n",
    "        deg = torch.zeros(N, device=x.device, dtype=x.dtype)\n",
    "        deg.index_add_(0, dst, torch.ones(src.size(0), device=x.device, dtype=x.dtype))\n",
    "        deg = deg.clamp(min=1)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        # Normalized message passing\n",
    "        norm = deg_inv_sqrt[src] * deg_inv_sqrt[dst]\n",
    "        h = self.linear(x)\n",
    "        agg = torch.zeros(N, h.size(1), device=x.device)\n",
    "        agg.index_add_(0, dst, h[src] * norm.unsqueeze(-1))\n",
    "        # Add self-loop\n",
    "        return agg + h\n",
    "\n",
    "\n",
    "class GINConv(nn.Module):\n",
    "    \"\"\"Graph Isomorphism Network layer (sum agg + MLP).\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, eps=0.0):\n",
    "        super().__init__()\n",
    "        self.eps = nn.Parameter(torch.tensor(eps))\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "        N = x.size(0)\n",
    "        agg = torch.zeros(N, x.size(1), device=x.device, dtype=x.dtype)\n",
    "        agg.index_add_(0, dst, x[src])\n",
    "        out = (1 + self.eps) * x + agg\n",
    "        return self.mlp(out)\n",
    "\n",
    "\n",
    "# ---------- Full Model Definitions ----------\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    \"\"\"2-layer GraphSAGE.\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.bn2(self.conv2(h, edge_index)))\n",
    "        h = self.dropout(h)\n",
    "        return self.classifier(h)\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        h = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        h = F.relu(self.bn2(self.conv2(h, edge_index)))\n",
    "        return h\n",
    "\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    \"\"\"2-layer GAT.\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConvLayer(in_dim, hidden_dim, num_heads, dropout)\n",
    "        self.conv2 = GATConvLayer(hidden_dim, hidden_dim, num_heads, dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.elu(self.bn1(self.conv1(x, edge_index)))\n",
    "        h = self.dropout(h)\n",
    "        h = F.elu(self.bn2(self.conv2(h, edge_index)))\n",
    "        h = self.dropout(h)\n",
    "        return self.classifier(h)\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        h = F.elu(self.bn1(self.conv1(x, edge_index)))\n",
    "        h = F.elu(self.bn2(self.conv2(h, edge_index)))\n",
    "        return h\n",
    "\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    \"\"\"2-layer GCN.\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.bn2(self.conv2(h, edge_index)))\n",
    "        h = self.dropout(h)\n",
    "        return self.classifier(h)\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        h = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        h = F.relu(self.bn2(self.conv2(h, edge_index)))\n",
    "        return h\n",
    "\n",
    "\n",
    "class GINModel(nn.Module):\n",
    "    \"\"\"2-layer GIN.\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GINConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GINConv(hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = self.dropout(h)\n",
    "        return self.classifier(h)\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        return h\n",
    "\n",
    "\n",
    "class HeteroGNNModel(nn.Module):\n",
    "    \"\"\"Multi-relation GNN that processes each edge type separately then fuses.\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_relations=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.relation_convs = nn.ModuleList([\n",
    "            SAGEConv(in_dim, hidden_dim) for _ in range(num_relations)\n",
    "        ])\n",
    "        self.relation_convs2 = nn.ModuleList([\n",
    "            SAGEConv(hidden_dim, hidden_dim) for _ in range(num_relations)\n",
    "        ])\n",
    "        self.fusion = nn.Linear(hidden_dim * num_relations, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bn1 = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_relations)])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.num_relations = num_relations\n",
    "\n",
    "    def forward(self, x, edge_indices_list):\n",
    "        \"\"\"edge_indices_list: list of edge_index tensors, one per relation type.\"\"\"\n",
    "        relation_outs = []\n",
    "        for i in range(min(self.num_relations, len(edge_indices_list))):\n",
    "            h = F.relu(self.bn1[i](self.relation_convs[i](x, edge_indices_list[i])))\n",
    "            h = self.dropout(h)\n",
    "            h = self.relation_convs2[i](h, edge_indices_list[i])\n",
    "            relation_outs.append(h)\n",
    "        # Pad if fewer edge types\n",
    "        while len(relation_outs) < self.num_relations:\n",
    "            relation_outs.append(torch.zeros_like(relation_outs[0]))\n",
    "        fused = torch.cat(relation_outs, dim=-1)\n",
    "        h = F.relu(self.bn2(self.fusion(fused)))\n",
    "        h = self.dropout(h)\n",
    "        return self.classifier(h)\n",
    "\n",
    "    def get_embeddings(self, x, edge_indices_list):\n",
    "        relation_outs = []\n",
    "        for i in range(min(self.num_relations, len(edge_indices_list))):\n",
    "            h = F.relu(self.bn1[i](self.relation_convs[i](x, edge_indices_list[i])))\n",
    "            h = self.relation_convs2[i](h, edge_indices_list[i])\n",
    "            relation_outs.append(h)\n",
    "        while len(relation_outs) < self.num_relations:\n",
    "            relation_outs.append(torch.zeros_like(relation_outs[0]))\n",
    "        fused = torch.cat(relation_outs, dim=-1)\n",
    "        return F.relu(self.bn2(self.fusion(fused)))\n",
    "\n",
    "print(f'All 6 GNN model classes defined.')\n",
    "print(f'Hidden dim from GPU config: {GPU_CONFIG[\"hidden_dim\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: GNN Training Engine (A100-optimized with AMP)\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_gnn(model, X, y, edge_index, is_hetero=False):\n",
    "    \"\"\"Evaluate model, returns metrics + probabilities.\"\"\"\n",
    "    model.eval()\n",
    "    amp_dtype = torch.bfloat16 if GPU_CONFIG['use_bf16'] else torch.float16\n",
    "    with torch.amp.autocast('cuda', enabled=GPU_CONFIG['use_amp'], dtype=amp_dtype):\n",
    "        logits = model(X, edge_index)\n",
    "        probs = F.softmax(logits.float(), dim=1)[:, 1].cpu().numpy()\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "    y_np = y.cpu().numpy()\n",
    "    return compute_metrics(y_np, preds, probs), probs\n",
    "\n",
    "\n",
    "def train_gnn_model(model_name, model, data, epochs=50, lr=0.005, patience=15, is_hetero=False):\n",
    "    \"\"\"Full GNN training loop with A100 AMP, early stopping, history tracking.\"\"\"\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training {model_name}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    print(f'  Device: {DEVICE} | Params: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "    # torch.compile for A100 (Linux only, requires Triton)\n",
    "    compiled_model = model\n",
    "    if GPU_CONFIG['use_compile'] and os.name != 'nt':\n",
    "        try:\n",
    "            compiled_model = torch.compile(model)\n",
    "            print('  torch.compile: applied')\n",
    "        except Exception as e:\n",
    "            print(f'  torch.compile: skipped ({e})')\n",
    "            compiled_model = model\n",
    "\n",
    "    # Class weights\n",
    "    n_pos = (data['y_train'] == 1).sum().float()\n",
    "    n_neg = (data['y_train'] == 0).sum().float()\n",
    "    weight = torch.tensor([1.0, (n_neg / n_pos).item()], device=DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    # Optimizer\n",
    "    try:\n",
    "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-4, fused=GPU_CONFIG['use_fused_adam'])\n",
    "    except TypeError:\n",
    "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=GPU_CONFIG['use_amp'])\n",
    "\n",
    "    # Edge data for hetero vs homogeneous\n",
    "    if is_hetero:\n",
    "        ei_train = data['edge_indices_train']\n",
    "        ei_val = data['edge_indices_val']\n",
    "        ei_test = data['edge_indices_test']\n",
    "    else:\n",
    "        ei_train = data['edge_index_train']\n",
    "        ei_val = data['edge_index_val']\n",
    "        ei_test = data['edge_index_test']\n",
    "\n",
    "    best_val_prauc = 0.0\n",
    "    best_epoch = 0\n",
    "    no_improve = 0\n",
    "    best_state = None\n",
    "    history = {'train_loss': [], 'val_prauc': [], 'val_f1': [], 'epoch': []}\n",
    "\n",
    "    t0 = time.time()\n",
    "    amp_dtype = torch.bfloat16 if GPU_CONFIG['use_bf16'] else torch.float16\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        compiled_model.train()\n",
    "        with torch.amp.autocast('cuda', enabled=GPU_CONFIG['use_amp'], dtype=amp_dtype):\n",
    "            logits = compiled_model(data['X_train'], ei_train)\n",
    "            loss = criterion(logits, data['y_train'])\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == epochs:\n",
    "            val_metrics, _ = evaluate_gnn(compiled_model, data['X_val'], data['y_val'], ei_val)\n",
    "            val_prauc = val_metrics.get('pr_auc', 0)\n",
    "            val_f1 = val_metrics.get('f1_sar', 0)\n",
    "            scheduler.step(val_prauc)\n",
    "\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_prauc'].append(val_prauc)\n",
    "            history['val_f1'].append(val_f1)\n",
    "            history['epoch'].append(epoch)\n",
    "\n",
    "            print(f'  Epoch {epoch:3d} | Loss: {train_loss:.4f} | Val PR-AUC: {val_prauc:.4f} | Val F1: {val_f1:.4f}')\n",
    "\n",
    "            if val_prauc > best_val_prauc:\n",
    "                best_val_prauc = val_prauc\n",
    "                best_epoch = epoch\n",
    "                best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "\n",
    "            if no_improve >= patience // 5:\n",
    "                print(f'  Early stopping at epoch {epoch} (best: {best_epoch})')\n",
    "                break\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    test_metrics, test_probs = evaluate_gnn(model, data['X_test'], data['y_test'], ei_test)\n",
    "    print_metrics(test_metrics, f'{model_name} Test ({elapsed:.1f}s)')\n",
    "\n",
    "    return test_metrics, model, history, test_probs\n",
    "\n",
    "print('GNN training engine ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Train ALL 5 GNN Models\n",
    "# ============================================================\n",
    "\n",
    "HIDDEN = GPU_CONFIG['hidden_dim']\n",
    "EPOCHS = 100\n",
    "LR = 0.005\n",
    "PATIENCE = 15\n",
    "\n",
    "gnn_results = {}\n",
    "gnn_models = {}\n",
    "gnn_histories = {}\n",
    "gnn_probs = {}\n",
    "\n",
    "# Prepare separate edge indices for HeteroGNN\n",
    "def get_hetero_edges(d, split_key):\n",
    "    \"\"\"Build list of edge indices for each relation type.\"\"\"\n",
    "    edges = []\n",
    "    for key in ['transfer_edge_index', 'same_wallet_edge_index', 'shared_device_edge_index']:\n",
    "        if key in d and d[key].shape[1] > 0:\n",
    "            edges.append(torch.tensor(d[key], dtype=torch.long).to(DEVICE))\n",
    "        else:\n",
    "            # Placeholder empty edges\n",
    "            edges.append(torch.zeros(2, 0, dtype=torch.long, device=DEVICE))\n",
    "    return edges\n",
    "\n",
    "hetero_data = dict(node_data)  # Copy base data\n",
    "hetero_data['edge_indices_train'] = get_hetero_edges(train_d, 'train')\n",
    "hetero_data['edge_indices_val'] = get_hetero_edges(val_d, 'val')\n",
    "hetero_data['edge_indices_test'] = get_hetero_edges(test_d, 'test')\n",
    "\n",
    "# 1. GraphSAGE\n",
    "print('\\n' + '#'*60)\n",
    "print('# MODEL 1/5: GraphSAGE')\n",
    "print('#'*60)\n",
    "sage = GraphSAGEModel(in_dim, HIDDEN, 2, dropout=0.3).to(DEVICE)\n",
    "m, sage, h, p = train_gnn_model('GraphSAGE', sage, node_data, EPOCHS, LR, PATIENCE)\n",
    "gnn_results['GraphSAGE'] = m; gnn_models['GraphSAGE'] = sage\n",
    "gnn_histories['GraphSAGE'] = h; gnn_probs['GraphSAGE'] = p\n",
    "\n",
    "# 2. GAT\n",
    "print('\\n' + '#'*60)\n",
    "print('# MODEL 2/5: GAT')\n",
    "print('#'*60)\n",
    "gat = GATModel(in_dim, max(HIDDEN // 2, 64), 2, num_heads=4, dropout=0.3).to(DEVICE)  # halved for memory safety\n",
    "m, gat, h, p = train_gnn_model('GAT', gat, node_data, EPOCHS, LR, PATIENCE)\n",
    "gnn_results['GAT'] = m; gnn_models['GAT'] = gat\n",
    "gnn_histories['GAT'] = h; gnn_probs['GAT'] = p\n",
    "\n",
    "# 3. GCN\n",
    "print('\\n' + '#'*60)\n",
    "print('# MODEL 3/5: GCN')\n",
    "print('#'*60)\n",
    "gcn = GCNModel(in_dim, HIDDEN, 2, dropout=0.3).to(DEVICE)\n",
    "m, gcn, h, p = train_gnn_model('GCN', gcn, node_data, EPOCHS, LR, PATIENCE)\n",
    "gnn_results['GCN'] = m; gnn_models['GCN'] = gcn\n",
    "gnn_histories['GCN'] = h; gnn_probs['GCN'] = p\n",
    "\n",
    "# 4. GIN\n",
    "print('\\n' + '#'*60)\n",
    "print('# MODEL 4/5: GIN')\n",
    "print('#'*60)\n",
    "gin = GINModel(in_dim, HIDDEN, 2, dropout=0.3).to(DEVICE)\n",
    "m, gin, h, p = train_gnn_model('GIN', gin, node_data, EPOCHS, LR, PATIENCE)\n",
    "gnn_results['GIN'] = m; gnn_models['GIN'] = gin\n",
    "gnn_histories['GIN'] = h; gnn_probs['GIN'] = p\n",
    "\n",
    "# 5. HeteroGNN\n",
    "print('\\n' + '#'*60)\n",
    "print('# MODEL 5/5: HeteroGNN (Multi-Relation)')\n",
    "print('#'*60)\n",
    "hetero = HeteroGNNModel(in_dim, HIDDEN, 2, num_relations=3, dropout=0.3).to(DEVICE)\n",
    "m, hetero, h, p = train_gnn_model('HeteroGNN', hetero, hetero_data, EPOCHS, LR, PATIENCE, is_hetero=True)\n",
    "gnn_results['HeteroGNN'] = m; gnn_models['HeteroGNN'] = hetero\n",
    "gnn_histories['HeteroGNN'] = h; gnn_probs['HeteroGNN'] = p\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print('\\n' + '='*60)\n",
    "print('ALL GNN MODELS TRAINED SUCCESSFULLY')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Save ALL Models in Multiple Formats\n",
    "\n",
    "Saving each GNN model in:\n",
    "- **PyTorch checkpoint (.pt)** - state_dict + config + metrics + normalization params\n",
    "- **TorchScript (.torchscript.pt)** - JIT-traced for C++/production deployment\n",
    "- **ONNX (.onnx)** - Cross-framework interoperability\n",
    "- **Full model pickle (.pkl)** - For quick Python reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Save ALL GNN Models in Multiple Formats\n",
    "# ============================================================\n",
    "\n",
    "def save_gnn_all_formats(name, model, metrics, data, is_hetero=False):\n",
    "    \"\"\"Save a GNN model in .pt, TorchScript, ONNX, and pickle formats.\"\"\"\n",
    "    safe_name = name.lower().replace(' ', '_')\n",
    "    model.eval()\n",
    "    model_cpu = model.cpu()\n",
    "    \n",
    "    # Determine config\n",
    "    config = {'in_dim': in_dim, 'hidden_dim': HIDDEN, 'out_dim': 2}\n",
    "    if name == 'GAT':\n",
    "        config['num_heads'] = 4\n",
    "    if name == 'HeteroGNN':\n",
    "        config['num_relations'] = 3\n",
    "    \n",
    "    norm_info = {\n",
    "        'mean': data['mean'].cpu().numpy().tolist(),\n",
    "        'std': data['std'].cpu().numpy().tolist()\n",
    "    }\n",
    "    \n",
    "    # 1. PyTorch Checkpoint (.pt)\n",
    "    pt_path = os.path.join(MODELS_DIR, f'{safe_name}_node.pt')\n",
    "    torch.save({\n",
    "        'model_state': model_cpu.state_dict(),\n",
    "        'config': config,\n",
    "        'metrics': metrics,\n",
    "        'norm': norm_info,\n",
    "        'model_class': name,\n",
    "        'feature_names': FEATURE_NAMES,\n",
    "        'hidden_dim': HIDDEN,\n",
    "        'training_info': {\n",
    "            'epochs': EPOCHS,\n",
    "            'lr': LR,\n",
    "            'device': GPU_CONFIG['gpu_name'],\n",
    "            'mig_profile': GPU_CONFIG['mig_profile'],\n",
    "        }\n",
    "    }, pt_path)\n",
    "    print(f'  [{name}] PyTorch checkpoint: {pt_path}')\n",
    "    \n",
    "    # 2. TorchScript (.torchscript.pt)\n",
    "    ts_path = os.path.join(MODELS_DIR, f'{safe_name}_node.torchscript.pt')\n",
    "    try:\n",
    "        # Create dummy inputs on CPU\n",
    "        dummy_x = torch.randn(100, in_dim)\n",
    "        if is_hetero:\n",
    "            dummy_edges = [torch.randint(0, 100, (2, 50)) for _ in range(3)]\n",
    "            traced = torch.jit.trace(model_cpu, (dummy_x, dummy_edges))\n",
    "        else:\n",
    "            dummy_edges = torch.randint(0, 100, (2, 200))\n",
    "            traced = torch.jit.trace(model_cpu, (dummy_x, dummy_edges))\n",
    "        traced.save(ts_path)\n",
    "        print(f'  [{name}] TorchScript: {ts_path}')\n",
    "    except Exception as e:\n",
    "        print(f'  [{name}] TorchScript FAILED: {e}')\n",
    "    \n",
    "    # 3. ONNX (.onnx)\n",
    "    onnx_path = os.path.join(MODELS_DIR, f'{safe_name}_node.onnx')\n",
    "    try:\n",
    "        dummy_x = torch.randn(100, in_dim)\n",
    "        if not is_hetero:\n",
    "            dummy_edges = torch.randint(0, 100, (2, 200))\n",
    "            torch.onnx.export(\n",
    "                model_cpu, (dummy_x, dummy_edges), onnx_path,\n",
    "                input_names=['node_features', 'edge_index'],\n",
    "                output_names=['logits'],\n",
    "                dynamic_axes={\n",
    "                    'node_features': {0: 'num_nodes'},\n",
    "                    'edge_index': {1: 'num_edges'},\n",
    "                    'logits': {0: 'num_nodes'}\n",
    "                },\n",
    "                opset_version=17\n",
    "            )\n",
    "            print(f'  [{name}] ONNX: {onnx_path}')\n",
    "        else:\n",
    "            print(f'  [{name}] ONNX: skipped (HeteroGNN has variable inputs)')\n",
    "    except Exception as e:\n",
    "        print(f'  [{name}] ONNX FAILED: {e}')\n",
    "    \n",
    "    # 4. Pickle (.pkl) - full model object\n",
    "    pkl_path = os.path.join(MODELS_DIR, f'{safe_name}_full.pkl')\n",
    "    try:\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'model': model_cpu,\n",
    "                'config': config,\n",
    "                'norm': norm_info,\n",
    "                'metrics': metrics\n",
    "            }, f)\n",
    "        print(f'  [{name}] Pickle: {pkl_path}')\n",
    "    except Exception as e:\n",
    "        print(f'  [{name}] Pickle FAILED: {e}')\n",
    "    \n",
    "    # Move model back to GPU\n",
    "    model.to(DEVICE)\n",
    "\n",
    "\n",
    "print('='*60)\n",
    "print('SAVING ALL GNN MODELS IN MULTIPLE FORMATS')\n",
    "print('='*60)\n",
    "\n",
    "for name, model in gnn_models.items():\n",
    "    is_h = (name == 'HeteroGNN')\n",
    "    save_gnn_all_formats(name, model, gnn_results[name], node_data, is_hetero=is_h)\n",
    "    print()\n",
    "\n",
    "# Save combined results JSON\n",
    "all_results = {\n",
    "    'baseline': baseline_results,\n",
    "    'gnn': {},\n",
    "    'gpu_config': GPU_CONFIG,\n",
    "    'training_config': {'epochs': EPOCHS, 'lr': LR, 'hidden_dim': HIDDEN, 'patience': PATIENCE}\n",
    "}\n",
    "\n",
    "def convert_np(obj):\n",
    "    if isinstance(obj, (np.integer,)): return int(obj)\n",
    "    elif isinstance(obj, (np.floating,)): return float(obj)\n",
    "    elif isinstance(obj, np.ndarray): return obj.tolist()\n",
    "    return obj\n",
    "\n",
    "for name, m in gnn_results.items():\n",
    "    all_results['gnn'][name] = m\n",
    "\n",
    "results_path = os.path.join(RESULTS_DIR, 'all_results.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2, default=convert_np)\n",
    "print(f'\\nCombined results: {results_path}')\n",
    "\n",
    "# Save training histories\n",
    "hist_path = os.path.join(RESULTS_DIR, 'gnn_training_histories.json')\n",
    "with open(hist_path, 'w') as f:\n",
    "    json.dump(gnn_histories, f, indent=2, default=convert_np)\n",
    "print(f'Training histories: {hist_path}')\n",
    "\n",
    "# List all saved models\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print('ALL SAVED MODEL FILES:')\n",
    "print(f'{\"=\"*60}')\n",
    "for f in sorted(os.listdir(MODELS_DIR)):\n",
    "    fpath = os.path.join(MODELS_DIR, f)\n",
    "    size_mb = os.path.getsize(fpath) / (1024*1024)\n",
    "    print(f'  {f:<45} {size_mb:>7.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Comprehensive Visualizations\n",
    "\n",
    "1. Training curves (loss, PR-AUC, F1 per epoch)\n",
    "2. Model comparison bar charts (all 9 models)\n",
    "3. ROC & PR curves\n",
    "4. Confusion matrices (heatmaps)\n",
    "5. Feature importance (XGBoost)\n",
    "6. GNN node embedding t-SNE\n",
    "7. Graph structure visualization\n",
    "8. Attention weight distribution (GAT)\n",
    "9. Class distribution & data overview\n",
    "10. Score distribution histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: VIZ 1 - GNN Training Curves\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(gnn_histories)))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0]\n",
    "for (name, hist), color in zip(gnn_histories.items(), colors):\n",
    "    ax.plot(hist['epoch'], hist['train_loss'], label=name, color=color, linewidth=2)\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Training Loss'); ax.set_title('Training Loss')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# PR-AUC\n",
    "ax = axes[1]\n",
    "for (name, hist), color in zip(gnn_histories.items(), colors):\n",
    "    ax.plot(hist['epoch'], hist['val_prauc'], label=name, color=color, linewidth=2, marker='o', markersize=3)\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Val PR-AUC'); ax.set_title('Validation PR-AUC')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# F1\n",
    "ax = axes[2]\n",
    "for (name, hist), color in zip(gnn_histories.items(), colors):\n",
    "    ax.plot(hist['epoch'], hist['val_f1'], label=name, color=color, linewidth=2, marker='s', markersize=3)\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Val F1 (SAR)'); ax.set_title('Validation F1 Score (SAR Class)')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('GNN Training Curves (A100 GPU)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_DIR, '01_training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 01_training_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: VIZ 2 - All Models Comparison Bar Chart\n",
    "# ============================================================\n",
    "\n",
    "# Combine all results\n",
    "all_model_names = []\n",
    "all_pr_auc = []\n",
    "all_roc_auc = []\n",
    "all_f1 = []\n",
    "\n",
    "for name, m in baseline_results.items():\n",
    "    all_model_names.append(f'BL:{name}')\n",
    "    all_pr_auc.append(m.get('pr_auc', 0))\n",
    "    all_roc_auc.append(m.get('roc_auc', 0))\n",
    "    all_f1.append(m.get('f1_sar', 0))\n",
    "\n",
    "for name, m in gnn_results.items():\n",
    "    all_model_names.append(f'GNN:{name}')\n",
    "    all_pr_auc.append(m.get('pr_auc', 0))\n",
    "    all_roc_auc.append(m.get('roc_auc', 0))\n",
    "    all_f1.append(m.get('f1_sar', 0))\n",
    "\n",
    "x = np.arange(len(all_model_names))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "bars1 = ax.bar(x - width, all_pr_auc, width, label='PR-AUC', color='#2196F3')\n",
    "bars2 = ax.bar(x, all_f1, width, label='F1 (SAR)', color='#FF9800')\n",
    "bars3 = ax.bar(x + width, all_roc_auc, width, label='ROC-AUC', color='#4CAF50')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Comparison: All Baselines vs All GNNs', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(all_model_names, rotation=45, ha='right', fontsize=10)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        if h > 0:\n",
    "            ax.annotate(f'{h:.3f}', xy=(bar.get_x() + bar.get_width()/2, h),\n",
    "                       xytext=(0, 3), textcoords='offset points', ha='center', fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_DIR, '02_model_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 02_model_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 14: VIZ 3 - ROC & PR Curves (all models on same plot)\n",
    "# ============================================================\n",
    "\n",
    "y_test_np = test_d['account_labels']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Collect all model probabilities\n",
    "all_probs = {}\n",
    "for name, bp in baseline_preds.items():\n",
    "    if name == 'y_test': continue\n",
    "    if bp.get('prob') is not None:\n",
    "        all_probs[f'BL:{name}'] = bp['prob']\n",
    "for name, p in gnn_probs.items():\n",
    "    all_probs[f'GNN:{name}'] = p\n",
    "\n",
    "colors_map = plt.cm.tab10(np.linspace(0, 1, len(all_probs)))\n",
    "\n",
    "# ROC Curve\n",
    "ax = axes[0]\n",
    "for (name, probs), color in zip(all_probs.items(), colors_map):\n",
    "    fpr, tpr, _ = roc_curve(y_test_np, probs)\n",
    "    auc_val = roc_auc_score(y_test_np, probs)\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC={auc_val:.4f})', color=color, linewidth=1.5)\n",
    "ax.plot([0,1], [0,1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('False Positive Rate'); ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves - All Models'); ax.legend(fontsize=8, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# PR Curve\n",
    "ax = axes[1]\n",
    "for (name, probs), color in zip(all_probs.items(), colors_map):\n",
    "    prec, rec, _ = precision_recall_curve(y_test_np, probs)\n",
    "    ap = average_precision_score(y_test_np, probs)\n",
    "    ax.plot(rec, prec, label=f'{name} (AP={ap:.4f})', color=color, linewidth=1.5)\n",
    "baseline_ratio = y_test_np.sum() / len(y_test_np)\n",
    "ax.axhline(y=baseline_ratio, color='gray', linestyle='--', alpha=0.5, label=f'Random ({baseline_ratio:.3f})')\n",
    "ax.set_xlabel('Recall'); ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall Curves - All Models'); ax.legend(fontsize=8, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ROC & PR Curves Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_DIR, '03_roc_pr_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 03_roc_pr_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 15: VIZ 4 - Confusion Matrices Heatmaps (GNN models)\n",
    "# ============================================================\n",
    "\n",
    "n_models = len(gnn_results)\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(4 * n_models, 4))\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (name, m) in zip(axes, gnn_results.items()):\n",
    "    cm = np.array(m.get('confusion_matrix', [[0,0],[0,0]]))\n",
    "    if HAS_SNS:\n",
    "        sns.heatmap(cm, annot=True, fmt=',d', cmap='Blues', ax=ax,\n",
    "                    xticklabels=['Normal', 'SAR'], yticklabels=['Normal', 'SAR'])\n",
    "    else:\n",
    "        im = ax.imshow(cm, cmap='Blues')\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                ax.text(j, i, f'{cm[i,j]:,}', ha='center', va='center', fontsize=10)\n",
    "        ax.set_xticks([0,1]); ax.set_xticklabels(['Normal','SAR'])\n",
    "        ax.set_yticks([0,1]); ax.set_yticklabels(['Normal','SAR'])\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('Actual')\n",
    "    ax.set_title(f'{name}\\nF1={m.get(\"f1_sar\",0):.3f}')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - GNN Models', fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_DIR, '04_confusion_matrices.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 04_confusion_matrices.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 16: VIZ 5 - Feature Importance (XGBoost)\n",
    "# ============================================================\n",
    "\n",
    "if HAS_XGB and 'xgboost' in baseline_models:\n",
    "    imp = baseline_models['xgboost'].feature_importances_\n",
    "    sorted_idx = np.argsort(imp)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    names = [FEATURE_NAMES[i] if i < len(FEATURE_NAMES) else f'feat_{i}' for i in sorted_idx]\n",
    "    ax.barh(names, imp[sorted_idx], color='#2196F3')\n",
    "    ax.set_xlabel('Feature Importance (Gain)')\n",
    "    ax.set_title('XGBoost Feature Importance for Mule Detection', fontsize=13, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, '05_feature_importance.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved: 05_feature_importance.png')\n",
    "else:\n",
    "    print('XGBoost not available, skipping feature importance plot.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 17: VIZ 6 - GNN Node Embedding t-SNE Visualization\n",
    "# ============================================================\n",
    "\n",
    "print('Computing GNN embeddings for t-SNE visualization...')\n",
    "print('(Using best model, sampling 5000 nodes for speed)')\n",
    "\n",
    "# Pick the best GNN by PR-AUC\n",
    "best_gnn_name = max(gnn_results, key=lambda k: gnn_results[k].get('pr_auc', 0))\n",
    "best_gnn = gnn_models[best_gnn_name]\n",
    "print(f'  Best GNN: {best_gnn_name}')\n",
    "\n",
    "# Get embeddings\n",
    "best_gnn.eval()\n",
    "with torch.no_grad():\n",
    "    if best_gnn_name == 'HeteroGNN':\n",
    "        emb = best_gnn.get_embeddings(node_data['X_test'], hetero_data['edge_indices_test'])\n",
    "    else:\n",
    "        emb = best_gnn.get_embeddings(node_data['X_test'], node_data['edge_index_test'])\n",
    "    emb_np = emb.cpu().numpy()\n",
    "\n",
    "labels_np = node_data['y_test'].cpu().numpy()\n",
    "\n",
    "# Sample for t-SNE (full dataset too large)\n",
    "n_sample = 5000\n",
    "sar_idx = np.where(labels_np == 1)[0]\n",
    "normal_idx = np.where(labels_np == 0)[0]\n",
    "# Ensure balanced sampling\n",
    "n_sar_sample = min(len(sar_idx), n_sample // 2)\n",
    "n_normal_sample = n_sample - n_sar_sample\n",
    "sample_idx = np.concatenate([\n",
    "    np.random.choice(sar_idx, n_sar_sample, replace=False),\n",
    "    np.random.choice(normal_idx, n_normal_sample, replace=False)\n",
    "])\n",
    "np.random.shuffle(sample_idx)\n",
    "\n",
    "print(f'  Running t-SNE on {len(sample_idx)} nodes...')\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "emb_2d = tsne.fit_transform(emb_np[sample_idx])\n",
    "labels_sample = labels_np[sample_idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter_normal = ax.scatter(\n",
    "    emb_2d[labels_sample == 0, 0], emb_2d[labels_sample == 0, 1],\n",
    "    c='#2196F3', alpha=0.3, s=10, label='Normal'\n",
    ")\n",
    "scatter_sar = ax.scatter(\n",
    "    emb_2d[labels_sample == 1, 0], emb_2d[labels_sample == 1, 1],\n",
    "    c='#F44336', alpha=0.6, s=25, label='SAR/Mule', edgecolors='darkred', linewidths=0.5\n",
    ")\n",
    "ax.set_xlabel('t-SNE 1'); ax.set_ylabel('t-SNE 2')\n",
    "ax.set_title(f'{best_gnn_name} Node Embeddings (t-SNE)\\n'\n",
    "             f'{n_sar_sample} SAR + {n_normal_sample} Normal accounts',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(markerscale=3, fontsize=11)\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_DIR, '06_tsne_embeddings.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 06_tsne_embeddings.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 18: VIZ 7 - Graph Structure Visualization (subgraph)\n",
    "# ============================================================\n",
    "\n",
    "if HAS_NX:\n",
    "    print('Visualizing transaction graph subgraph around a SAR node...')\n",
    "    \n",
    "    edges = test_d['transfer_edge_index']\n",
    "    labels_np = test_d['account_labels']\n",
    "    \n",
    "    # Find a SAR node with many connections\n",
    "    sar_nodes = np.where(labels_np == 1)[0]\n",
    "    # Count edges per SAR node\n",
    "    sar_degrees = []\n",
    "    for s in sar_nodes[:200]:  # Check first 200\n",
    "        deg = np.sum(edges[0] == s) + np.sum(edges[1] == s)\n",
    "        sar_degrees.append((s, deg))\n",
    "    sar_degrees.sort(key=lambda x: -x[1])\n",
    "    center_node = sar_degrees[0][0]\n",
    "    \n",
    "    # Build 2-hop subgraph\n",
    "    G = nx.DiGraph()\n",
    "    visited = {center_node}\n",
    "    frontier = {center_node}\n",
    "    \n",
    "    for hop in range(2):\n",
    "        next_frontier = set()\n",
    "        for i in range(min(edges.shape[1], 200000)):\n",
    "            src, dst = int(edges[0, i]), int(edges[1, i])\n",
    "            if src in frontier:\n",
    "                next_frontier.add(dst)\n",
    "                G.add_edge(src, dst)\n",
    "            if dst in frontier:\n",
    "                next_frontier.add(src)\n",
    "                G.add_edge(src, dst)\n",
    "        frontier = next_frontier - visited\n",
    "        visited.update(frontier)\n",
    "        if len(visited) > 300:\n",
    "            break\n",
    "    \n",
    "    # Limit to 200 nodes for readability\n",
    "    if len(G.nodes) > 200:\n",
    "        nodes_to_keep = list(G.nodes)[:200]\n",
    "        G = G.subgraph(nodes_to_keep).copy()\n",
    "    \n",
    "    # Color nodes\n",
    "    node_colors = []\n",
    "    node_sizes = []\n",
    "    for n in G.nodes:\n",
    "        if n == center_node:\n",
    "            node_colors.append('#FF0000')\n",
    "            node_sizes.append(200)\n",
    "        elif n < len(labels_np) and labels_np[n] == 1:\n",
    "            node_colors.append('#FF6B6B')\n",
    "            node_sizes.append(80)\n",
    "        else:\n",
    "            node_colors.append('#64B5F6')\n",
    "            node_sizes.append(30)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    pos = nx.spring_layout(G, k=0.3, iterations=50, seed=42)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.15, edge_color='gray', arrows=True,\n",
    "                           arrowsize=5, ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                           alpha=0.8, ax=ax)\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(color='#FF0000', label=f'Center SAR Node ({center_node})'),\n",
    "        mpatches.Patch(color='#FF6B6B', label='SAR/Mule Accounts'),\n",
    "        mpatches.Patch(color='#64B5F6', label='Normal Accounts'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "    ax.set_title(f'Transaction Graph: 2-hop Neighborhood of SAR Account {center_node}\\n'\n",
    "                 f'{len(G.nodes)} nodes, {len(G.edges)} edges',\n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, '07_graph_structure.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved: 07_graph_structure.png')\n",
    "else:\n",
    "    print('networkx not available, skipping graph visualization.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 19: VIZ 8 - Score Distribution & Edge Type Analysis\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 8a: Score distribution for best GNN\n",
    "ax = axes[0, 0]\n",
    "best_probs = gnn_probs[best_gnn_name]\n",
    "sar_mask = test_d['account_labels'] == 1\n",
    "ax.hist(best_probs[~sar_mask], bins=50, alpha=0.7, label='Normal', color='#2196F3', density=True)\n",
    "ax.hist(best_probs[sar_mask], bins=50, alpha=0.7, label='SAR/Mule', color='#F44336', density=True)\n",
    "ax.axvline(x=0.5, color='black', linestyle='--', alpha=0.5, label='Threshold=0.5')\n",
    "ax.set_xlabel('Predicted SAR Probability')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f'{best_gnn_name} Score Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 8b: Edge type distribution\n",
    "ax = axes[0, 1]\n",
    "edge_types = []\n",
    "edge_counts = []\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    stats_path = os.path.join(BASE_DIR, split_name, 'graph_data', 'graph_stats.json')\n",
    "    if os.path.exists(stats_path):\n",
    "        with open(stats_path) as f:\n",
    "            stats = json.load(f)\n",
    "        if split_name == 'test':\n",
    "            for etype, count in stats['edges'].items():\n",
    "                if count > 0:\n",
    "                    edge_types.append(etype)\n",
    "                    edge_counts.append(count)\n",
    "\n",
    "if edge_types:\n",
    "    y_pos = np.arange(len(edge_types))\n",
    "    colors_edges = plt.cm.Set2(np.linspace(0, 1, len(edge_types)))\n",
    "    ax.barh(y_pos, edge_counts, color=colors_edges)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(edge_types, fontsize=9)\n",
    "    ax.set_xlabel('Number of Edges')\n",
    "    ax.set_title('Edge Type Distribution (Test Split)')\n",
    "    for i, v in enumerate(edge_counts):\n",
    "        ax.text(v + max(edge_counts)*0.01, i, f'{v:,}', va='center', fontsize=8)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 8c: Class distribution across splits\n",
    "ax = axes[1, 0]\n",
    "splits_names = ['train', 'val', 'test']\n",
    "sar_counts = []\n",
    "normal_counts = []\n",
    "for split_name in splits_names:\n",
    "    labels = np.load(os.path.join(BASE_DIR, split_name, 'graph_data', 'account_labels.npy'))\n",
    "    sar_counts.append(labels.sum())\n",
    "    normal_counts.append(len(labels) - labels.sum())\n",
    "\n",
    "x_pos = np.arange(len(splits_names))\n",
    "ax.bar(x_pos - 0.2, normal_counts, 0.4, label='Normal', color='#2196F3')\n",
    "ax.bar(x_pos + 0.2, sar_counts, 0.4, label='SAR', color='#F44336')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(splits_names)\n",
    "ax.set_ylabel('Number of Accounts')\n",
    "ax.set_title('Class Distribution Across Splits')\n",
    "ax.legend()\n",
    "for i in range(len(splits_names)):\n",
    "    ax.text(i - 0.2, normal_counts[i] + 2000, f'{normal_counts[i]:,}', ha='center', fontsize=8)\n",
    "    ax.text(i + 0.2, sar_counts[i] + 2000, f'{sar_counts[i]:,}', ha='center', fontsize=8)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 8d: Recall@Kx comparison\n",
    "ax = axes[1, 1]\n",
    "model_names_rk = []\n",
    "recall_1x = []\n",
    "recall_2x = []\n",
    "recall_5x = []\n",
    "for name, m in {**baseline_results, **gnn_results}.items():\n",
    "    if 'recall@1x' in m:\n",
    "        model_names_rk.append(name)\n",
    "        recall_1x.append(m.get('recall@1x', 0))\n",
    "        recall_2x.append(m.get('recall@2x', 0))\n",
    "        recall_5x.append(m.get('recall@5x', 0))\n",
    "\n",
    "if model_names_rk:\n",
    "    x_rk = np.arange(len(model_names_rk))\n",
    "    w = 0.25\n",
    "    ax.bar(x_rk - w, recall_1x, w, label='Recall@1x', color='#FF5722')\n",
    "    ax.bar(x_rk, recall_2x, w, label='Recall@2x', color='#FF9800')\n",
    "    ax.bar(x_rk + w, recall_5x, w, label='Recall@5x', color='#FFC107')\n",
    "    ax.set_xticks(x_rk)\n",
    "    ax.set_xticklabels(model_names_rk, rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_ylabel('Recall')\n",
    "    ax.set_title('Recall@Kx (Top-K Retrieval Quality)')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Data & Score Analysis', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_DIR, '08_score_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 08_score_analysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 20: VIZ 9 - GAT Attention Weights Analysis\n",
    "# ============================================================\n",
    "\n",
    "print('Analyzing GAT attention weight distribution...')\n",
    "\n",
    "gat_model = gnn_models.get('GAT')\n",
    "if gat_model is not None:\n",
    "    gat_model.eval()\n",
    "    X_test_t = node_data['X_test']\n",
    "    ei_test = node_data['edge_index_test']\n",
    "    \n",
    "    # Extract attention from first GAT layer\n",
    "    with torch.no_grad():\n",
    "        N = X_test_t.size(0)\n",
    "        src, dst = ei_test[0], ei_test[1]\n",
    "        conv1 = gat_model.conv1\n",
    "        h = conv1.W(X_test_t).view(N, conv1.num_heads, conv1.head_dim)\n",
    "        score_src = (h * conv1.a_src).sum(dim=-1)\n",
    "        score_dst = (h * conv1.a_dst).sum(dim=-1)\n",
    "        e = F.leaky_relu(score_src[src] + score_dst[dst], 0.2)\n",
    "        e_max = torch.zeros(N, conv1.num_heads, device=DEVICE)\n",
    "        e_max.index_reduce_(0, dst, e, 'amax', include_self=True)\n",
    "        e_exp = torch.exp(e - e_max[dst])\n",
    "        e_sum = torch.zeros(N, conv1.num_heads, device=DEVICE)\n",
    "        e_sum.index_add_(0, dst, e_exp)\n",
    "        attention = (e_exp / e_sum[dst].clamp(min=1e-9)).cpu().numpy()\n",
    "    \n",
    "    labels_test = node_data['y_test'].cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Attention distribution per head\n",
    "    ax = axes[0]\n",
    "    for head in range(min(4, attention.shape[1])):\n",
    "        ax.hist(attention[:, head], bins=50, alpha=0.5, label=f'Head {head+1}', density=True)\n",
    "    ax.set_xlabel('Attention Weight')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('Attention Weight Distribution (Layer 1)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Attention on SAR vs Normal edges\n",
    "    ax = axes[1]\n",
    "    # Edges where destination is SAR\n",
    "    sar_dst_mask = labels_test[dst.cpu().numpy()] == 1\n",
    "    normal_dst_mask = ~sar_dst_mask\n",
    "    mean_att = attention.mean(axis=1)  # Average across heads\n",
    "    \n",
    "    ax.hist(mean_att[normal_dst_mask], bins=50, alpha=0.6, label='To Normal', color='#2196F3', density=True)\n",
    "    ax.hist(mean_att[sar_dst_mask], bins=50, alpha=0.6, label='To SAR', color='#F44336', density=True)\n",
    "    ax.set_xlabel('Mean Attention Weight')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('Attention: Edges to SAR vs Normal Nodes')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Heatmap: mean attention by head for SAR vs Normal\n",
    "    ax = axes[2]\n",
    "    att_sar_mean = attention[sar_dst_mask].mean(axis=0)\n",
    "    att_normal_mean = attention[normal_dst_mask].mean(axis=0)\n",
    "    att_matrix = np.stack([att_normal_mean, att_sar_mean])\n",
    "    \n",
    "    if HAS_SNS:\n",
    "        sns.heatmap(att_matrix, annot=True, fmt='.4f', cmap='YlOrRd', ax=ax,\n",
    "                    xticklabels=[f'Head {i+1}' for i in range(att_matrix.shape[1])],\n",
    "                    yticklabels=['Normal', 'SAR'])\n",
    "    else:\n",
    "        im = ax.imshow(att_matrix, cmap='YlOrRd', aspect='auto')\n",
    "        for i in range(2):\n",
    "            for j in range(att_matrix.shape[1]):\n",
    "                ax.text(j, i, f'{att_matrix[i,j]:.4f}', ha='center', va='center')\n",
    "        ax.set_xticks(range(att_matrix.shape[1]))\n",
    "        ax.set_xticklabels([f'Head {i+1}' for i in range(att_matrix.shape[1])])\n",
    "        ax.set_yticks([0,1])\n",
    "        ax.set_yticklabels(['Normal', 'SAR'])\n",
    "    ax.set_title('Mean Attention by Head\\n(Edges to SAR vs Normal)')\n",
    "    \n",
    "    plt.suptitle('GAT Attention Analysis', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, '09_gat_attention.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved: 09_gat_attention.png')\n",
    "else:\n",
    "    print('GAT model not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 21: VIZ 10 - Feature Correlation & Embedding Comparison\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# 10a: Feature correlation heatmap\n",
    "ax = axes[0]\n",
    "features_test = test_d['account_features']\n",
    "feat_df = pd.DataFrame(features_test, columns=FEATURE_NAMES[:features_test.shape[1]])\n",
    "corr = feat_df.corr()\n",
    "if HAS_SNS:\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0, ax=ax,\n",
    "                square=True, linewidths=0.5, cbar_kws={'shrink': 0.8})\n",
    "else:\n",
    "    im = ax.imshow(corr.values, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    ax.set_xticks(range(len(corr.columns)))\n",
    "    ax.set_xticklabels(corr.columns, rotation=90, fontsize=7)\n",
    "    ax.set_yticks(range(len(corr.columns)))\n",
    "    ax.set_yticklabels(corr.columns, fontsize=7)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "ax.set_title('Account Feature Correlations', fontsize=12, fontweight='bold')\n",
    "ax.tick_params(labelsize=7)\n",
    "\n",
    "# 10b: t-SNE comparison of all GNN embeddings (2x3 subplot would be too complex, use overlay)\n",
    "ax = axes[1]\n",
    "# Compare top 3 GNN models by PR-AUC\n",
    "sorted_gnns = sorted(gnn_results.items(), key=lambda x: x[1].get('pr_auc', 0), reverse=True)[:3]\n",
    "colors_emb = ['#F44336', '#2196F3', '#4CAF50']\n",
    "\n",
    "for i, (name, _) in enumerate(sorted_gnns):\n",
    "    model_i = gnn_models[name]\n",
    "    model_i.eval()\n",
    "    with torch.no_grad():\n",
    "        if name == 'HeteroGNN':\n",
    "            emb_i = model_i.get_embeddings(node_data['X_test'], hetero_data['edge_indices_test'])\n",
    "        else:\n",
    "            emb_i = model_i.get_embeddings(node_data['X_test'], node_data['edge_index_test'])\n",
    "    emb_i_np = emb_i.cpu().numpy()\n",
    "    \n",
    "    # Sample just SAR nodes for clarity\n",
    "    sar_idx_i = np.where(test_d['account_labels'] == 1)[0]\n",
    "    sample_i = np.random.choice(sar_idx_i, min(500, len(sar_idx_i)), replace=False)\n",
    "    \n",
    "    tsne_i = TSNE(n_components=2, random_state=42 + i, perplexity=20, n_iter=500)\n",
    "    emb_2d_i = tsne_i.fit_transform(emb_i_np[sample_i])\n",
    "    \n",
    "    ax.scatter(emb_2d_i[:, 0], emb_2d_i[:, 1], c=colors_emb[i], alpha=0.4, s=15,\n",
    "               label=f'{name} (SAR nodes)')\n",
    "\n",
    "ax.set_xlabel('t-SNE 1'); ax.set_ylabel('t-SNE 2')\n",
    "ax.set_title('SAR Node Embeddings: Top 3 GNN Models', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_DIR, '10_feature_embedding_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 10_feature_embedding_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Final Summary & GPU Memory Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 22: Final Summary Table & GPU Report\n",
    "# ============================================================\n",
    "\n",
    "print('='*80)\n",
    "print('                    GHOSTBUSTERS - FINAL RESULTS SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "# Combined results table\n",
    "print(f'\\n{\"Model\":<25} {\"Type\":<10} {\"PR-AUC\":>8} {\"ROC-AUC\":>8} {\"F1(SAR)\":>8} {\"Prec\":>8} {\"Recall\":>8} {\"R@1x\":>6}')\n",
    "print('-'*80)\n",
    "\n",
    "for name, m in baseline_results.items():\n",
    "    print(f'{name:<25} {\"Baseline\":<10} {m.get(\"pr_auc\",0):>8.4f} {m.get(\"roc_auc\",0):>8.4f} '\n",
    "          f'{m.get(\"f1_sar\",0):>8.4f} {m.get(\"precision_sar\",0):>8.4f} '\n",
    "          f'{m.get(\"recall_sar\",0):>8.4f} {m.get(\"recall@1x\",0):>6.3f}')\n",
    "\n",
    "print('-'*80)\n",
    "\n",
    "for name, m in gnn_results.items():\n",
    "    print(f'{name:<25} {\"GNN\":<10} {m.get(\"pr_auc\",0):>8.4f} {m.get(\"roc_auc\",0):>8.4f} '\n",
    "          f'{m.get(\"f1_sar\",0):>8.4f} {m.get(\"precision_sar\",0):>8.4f} '\n",
    "          f'{m.get(\"recall_sar\",0):>8.4f} {m.get(\"recall@1x\",0):>6.3f}')\n",
    "\n",
    "# Best models\n",
    "print(f'\\n{\"=\"*80}')\n",
    "best_bl_name = max(baseline_results, key=lambda k: baseline_results[k].get('pr_auc', 0))\n",
    "best_gn_name = max(gnn_results, key=lambda k: gnn_results[k].get('pr_auc', 0))\n",
    "print(f'Best Baseline: {best_bl_name} (PR-AUC: {baseline_results[best_bl_name].get(\"pr_auc\",0):.4f})')\n",
    "print(f'Best GNN:      {best_gn_name} (PR-AUC: {gnn_results[best_gn_name].get(\"pr_auc\",0):.4f})')\n",
    "\n",
    "# GPU memory summary\n",
    "if torch.cuda.is_available():\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print('GPU MEMORY REPORT')\n",
    "    print(f'{\"=\"*80}')\n",
    "    print(f'  GPU: {GPU_CONFIG[\"gpu_name\"]}')\n",
    "    print(f'  MIG Profile: {GPU_CONFIG[\"mig_profile\"]}')\n",
    "    print(f'  Peak Memory Allocated: {torch.cuda.max_memory_allocated()/1024**3:.2f} GB')\n",
    "    print(f'  Peak Memory Reserved:  {torch.cuda.max_memory_reserved()/1024**3:.2f} GB')\n",
    "    print(f'  Current Memory:        {torch.cuda.memory_allocated()/1024**3:.2f} GB')\n",
    "\n",
    "# Saved files summary\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print('SAVED FILES')\n",
    "print(f'{\"=\"*80}')\n",
    "\n",
    "print('\\nModels:')\n",
    "for f in sorted(os.listdir(MODELS_DIR)):\n",
    "    fpath = os.path.join(MODELS_DIR, f)\n",
    "    size_mb = os.path.getsize(fpath) / (1024*1024)\n",
    "    print(f'  models/{f:<42} {size_mb:>7.2f} MB')\n",
    "\n",
    "print('\\nResults:')\n",
    "for f in sorted(os.listdir(RESULTS_DIR)):\n",
    "    fpath = os.path.join(RESULTS_DIR, f)\n",
    "    size_mb = os.path.getsize(fpath) / (1024*1024)\n",
    "    print(f'  results/{f:<41} {size_mb:>7.2f} MB')\n",
    "\n",
    "print('\\nVisualizations:')\n",
    "for f in sorted(os.listdir(VIZ_DIR)):\n",
    "    fpath = os.path.join(VIZ_DIR, f)\n",
    "    size_mb = os.path.getsize(fpath) / (1024*1024)\n",
    "    print(f'  visualizations/{f:<34} {size_mb:>7.2f} MB')\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print('PIPELINE COMPLETE - All models trained, saved, and visualized.')\n",
    "print(f'{\"=\"*80}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}